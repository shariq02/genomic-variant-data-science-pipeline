{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c47d3a-1b8e-4c54-ace8-96883986aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# ML MODEL TRAINING\n",
    "# DNA Gene Mapping Project\n",
    "# Author: Sharique Mohammad\n",
    "# Date: 12 January 2026\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526abefb-5974-4bb7-90ba-f62ba6dbba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4ddb6-a5e5-4d62-a92b-cceaeb42e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'jupyter_notebooks' else Path.cwd()\n",
    "ML_DIR = PROJECT_ROOT / \"data\" / \"ml\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "VIZ_DIR = PROJECT_ROOT / \"data\" / \"analytical\" / \"visualizations\"\n",
    "\n",
    "ML_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIZ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"ML directory: {ML_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7e602-e73b-48f4-b9b4-63382bfeac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection & Load Data\n",
    "\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', 'localhost'),\n",
    "    'port': int(os.getenv('POSTGRES_PORT', 5432)),\n",
    "    'database': os.getenv('POSTGRES_DATABASE', 'genome_db'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD')\n",
    "}\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}\"\n",
    "    f\"@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "print(\"Loading ML features from PostgreSQL...\")\n",
    "df_ml = pd.read_sql(\"SELECT * FROM gold.ml_features\", engine)\n",
    "print(f\"Loaded {len(df_ml)} genes with ML features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cb168-ad2c-46ff-a305-414c48d48134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ML FEATURES OVERVIEW\")\n",
    "print(\"=\"*40)\n",
    "print(df_ml.head())\n",
    "print(\"\\nShape:\", df_ml.shape)\n",
    "print(\"\\nColumns:\", df_ml.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(df_ml.dtypes)\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(df_ml['risk_level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e4d0f-5c1f-4d6d-af7f-786937552811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering & Preparation\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Remove genes with NULL chromosome\n",
    "df_ml_clean = df_ml[df_ml['chromosome'].notna()].copy()\n",
    "print(f\"Removed {len(df_ml) - len(df_ml_clean)} genes with NULL chromosome\")\n",
    "\n",
    "# Create additional features\n",
    "df_ml_clean['benign_ratio'] = df_ml_clean['total_benign'] / df_ml_clean['mutation_count']\n",
    "df_ml_clean['pathogenic_to_benign'] = df_ml_clean['total_pathogenic'] / (df_ml_clean['total_benign'] + 1)\n",
    "df_ml_clean['has_diseases'] = (df_ml_clean['disease_count'] > 0).astype(int)\n",
    "df_ml_clean['high_mutation_density'] = (df_ml_clean['mutation_density'] > df_ml_clean['mutation_density'].median()).astype(int)\n",
    "\n",
    "print(\"Created additional features:\")\n",
    "print(\"  - benign_ratio\")\n",
    "print(\"  - pathogenic_to_benign\")\n",
    "print(\"  - has_diseases\")\n",
    "print(\"  - high_mutation_density\")\n",
    "\n",
    "# One-hot encode chromosome\n",
    "df_encoded = pd.get_dummies(df_ml_clean, columns=['chromosome'], prefix='chr', drop_first=True)\n",
    "print(f\"\\nOne-hot encoded chromosome (created {len([c for c in df_encoded.columns if c.startswith('chr')])} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0a0bd-6042-4f2a-961d-9d157a0db13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Prepare Features and Target\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"PREPARING FEATURES AND TARGET\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Define feature columns (exclude target and identifiers)\n",
    "exclude_cols = ['gene_name', 'risk_level', 'created_at']\n",
    "feature_cols = [col for col in df_encoded.columns if col not in exclude_cols]\n",
    "\n",
    "X = df_encoded[feature_cols].copy()\n",
    "y = df_encoded['risk_level'].copy()\n",
    "\n",
    "print(f\"Features: {len(feature_cols)} columns\")\n",
    "print(f\"Target: risk_level\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_cols[:10], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "if len(feature_cols) > 10:\n",
    "    print(f\"  ... and {len(feature_cols) - 10} more\")\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\nTarget proportions:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acb430-9447-41b8-a504-39d75b5341df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Train/Val/Test Split\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"TRAIN/VAL/TEST SPLIT\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(f\"Encoded classes: {le.classes_}\")\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Val set:   {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTrain set class distribution:\")\n",
    "train_dist = pd.Series(y_train).value_counts()\n",
    "for i, count in train_dist.items():\n",
    "    print(f\"  {le.classes_[i]}: {count} ({count/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001d786-e9bb-4208-af4e-600023966d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Feature Scaling\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Applied StandardScaler to all features\")\n",
    "print(f\"Feature mean (after scaling): {X_train_scaled.mean():.4f}\")\n",
    "print(f\"Feature std (after scaling): {X_train_scaled.std():.4f}\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, MODELS_DIR / 'scaler.pkl')\n",
    "joblib.dump(le, MODELS_DIR / 'label_encoder.pkl')\n",
    "print(f\"\\nSaved scaler to: {MODELS_DIR / 'scaler.pkl'}\")\n",
    "print(f\"Saved label encoder to: {MODELS_DIR / 'label_encoder.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672edab0-f23a-4c20-b0a4-1226a5d66bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - Logistic Regression (Baseline)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION (BASELINE)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_train_pred = lr_model.predict(X_train_scaled)\n",
    "lr_val_pred = lr_model.predict(X_val_scaled)\n",
    "\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_pred)\n",
    "lr_val_acc = accuracy_score(y_val, lr_val_pred)\n",
    "lr_train_f1 = f1_score(y_train, lr_train_pred, average='weighted')\n",
    "lr_val_f1 = f1_score(y_val, lr_val_pred, average='weighted')\n",
    "\n",
    "print(f\"Train Accuracy: {lr_train_acc:.4f}\")\n",
    "print(f\"Train F1-Score: {lr_train_f1:.4f}\")\n",
    "print(f\"Val Accuracy:   {lr_val_acc:.4f}\")\n",
    "print(f\"Val F1-Score:   {lr_val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, lr_val_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f09f4-6291-4cef-9342-b7bf9cb78c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - Random Forest\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL 2: RANDOM FOREST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "rf_train_acc = accuracy_score(y_train, rf_train_pred)\n",
    "rf_val_acc = accuracy_score(y_val, rf_val_pred)\n",
    "rf_train_f1 = f1_score(y_train, rf_train_pred, average='weighted')\n",
    "rf_val_f1 = f1_score(y_val, rf_val_pred, average='weighted')\n",
    "\n",
    "print(f\"Train Accuracy: {rf_train_acc:.4f}\")\n",
    "print(f\"Train F1-Score: {rf_train_f1:.4f}\")\n",
    "print(f\"Val Accuracy:   {rf_val_acc:.4f}\")\n",
    "print(f\"Val F1-Score:   {rf_val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, rf_val_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf1333-d546-4949-8d70-0540d567154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 - XGBoost\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL 3: XGBOOST\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "xgb_train_acc = accuracy_score(y_train, xgb_train_pred)\n",
    "xgb_val_acc = accuracy_score(y_val, xgb_val_pred)\n",
    "xgb_train_f1 = f1_score(y_train, xgb_train_pred, average='weighted')\n",
    "xgb_val_f1 = f1_score(y_val, xgb_val_pred, average='weighted')\n",
    "\n",
    "print(f\"Train Accuracy: {xgb_train_acc:.4f}\")\n",
    "print(f\"Train F1-Score: {xgb_train_f1:.4f}\")\n",
    "print(f\"Val Accuracy:   {xgb_val_acc:.4f}\")\n",
    "print(f\"Val F1-Score:   {xgb_val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, xgb_val_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36006ea2-4031-4123-ab1e-bf76b136a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 - LightGBM\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL 4: LIGHTGBM\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "lgbm_model = LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "lgbm_train_pred = lgbm_model.predict(X_train)\n",
    "lgbm_val_pred = lgbm_model.predict(X_val)\n",
    "\n",
    "lgbm_train_acc = accuracy_score(y_train, lgbm_train_pred)\n",
    "lgbm_val_acc = accuracy_score(y_val, lgbm_val_pred)\n",
    "lgbm_train_f1 = f1_score(y_train, lgbm_train_pred, average='weighted')\n",
    "lgbm_val_f1 = f1_score(y_val, lgbm_val_pred, average='weighted')\n",
    "\n",
    "print(f\"Train Accuracy: {lgbm_train_acc:.4f}\")\n",
    "print(f\"Train F1-Score: {lgbm_train_f1:.4f}\")\n",
    "print(f\"Val Accuracy:   {lgbm_val_acc:.4f}\")\n",
    "print(f\"Val F1-Score:   {lgbm_val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, lgbm_val_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff012b-8e0c-497b-964d-a5799dead84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'LightGBM'],\n",
    "    'Train_Accuracy': [lr_train_acc, rf_train_acc, xgb_train_acc, lgbm_train_acc],\n",
    "    'Val_Accuracy': [lr_val_acc, rf_val_acc, xgb_val_acc, lgbm_val_acc],\n",
    "    'Train_F1': [lr_train_f1, rf_train_f1, xgb_train_f1, lgbm_train_f1],\n",
    "    'Val_F1': [lr_val_f1, rf_val_f1, xgb_val_f1, lgbm_val_f1],\n",
    "    'Overfit_Gap': [\n",
    "        lr_train_f1 - lr_val_f1,\n",
    "        rf_train_f1 - rf_val_f1,\n",
    "        xgb_train_f1 - xgb_val_f1,\n",
    "        lgbm_train_f1 - lgbm_val_f1\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Val_F1', ascending=False)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "comparison_df.plot(x='Model', y=['Train_Accuracy', 'Val_Accuracy'], \n",
    "                   kind='bar', ax=axes[0], rot=45)\n",
    "axes[0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend(['Train', 'Validation'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1-Score comparison\n",
    "comparison_df.plot(x='Model', y=['Train_F1', 'Val_F1'], \n",
    "                   kind='bar', ax=axes[1], rot=45, color=['green', 'orange'])\n",
    "axes[1].set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('F1-Score (Weighted)')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].legend(['Train', 'Validation'])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_val_f1 = comparison_df.iloc[0]['Val_F1']\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best Val F1-Score: {best_val_f1:.4f}\")\n",
    "\n",
    "# Map to actual model object\n",
    "model_map = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model,\n",
    "    'LightGBM': lgbm_model\n",
    "}\n",
    "best_model = model_map[best_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25a56f-3e43-4dc9-9de4-ee1863a82fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning (Best Model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"HYPERPARAMETER TUNING: {best_model_name}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if best_model_name == 'LightGBM':\n",
    "    param_grid = {\n",
    "        'num_leaves': [20, 31, 50],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [5, 7, 10],\n",
    "        'min_child_samples': [10, 20, 30]\n",
    "    }\n",
    "    base_model = LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
    "    \n",
    "elif best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "elif best_model_name == 'XGBoost':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    }\n",
    "    base_model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss')\n",
    "else:\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs', 'saga']\n",
    "    }\n",
    "    base_model = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "print(f\"Parameter grid: {len(list(param_grid.values())[0]) ** len(param_grid)} combinations\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    base_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Use scaled data for Logistic Regression, raw for tree-based\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "else:\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV F1-Score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "final_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c84d9-9e64-4071-ab09-150f1d8c7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Predictions on all sets\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    final_train_pred = final_model.predict(X_train_scaled)\n",
    "    final_val_pred = final_model.predict(X_val_scaled)\n",
    "    final_test_pred = final_model.predict(X_test_scaled)\n",
    "else:\n",
    "    final_train_pred = final_model.predict(X_train)\n",
    "    final_val_pred = final_model.predict(X_val)\n",
    "    final_test_pred = final_model.predict(X_test)\n",
    "\n",
    "final_train_f1 = f1_score(y_train, final_train_pred, average='weighted')\n",
    "final_val_f1 = f1_score(y_val, final_val_pred, average='weighted')\n",
    "final_test_f1 = f1_score(y_test, final_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Train F1-Score: {final_train_f1:.4f}\")\n",
    "print(f\"Val F1-Score:   {final_val_f1:.4f}\")\n",
    "print(f\"Test F1-Score:  {final_test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, final_test_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2808c85-1339-4c9d-ba74-164929058b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SAVING FINAL MODEL\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(final_model, MODELS_DIR / 'best_model.pkl')\n",
    "print(f\"Saved model to: {MODELS_DIR / 'best_model.pkl'}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names = {'features': feature_cols}\n",
    "pd.DataFrame([feature_names]).to_json(MODELS_DIR / 'feature_names.json', orient='records')\n",
    "print(f\"Saved feature names to: {MODELS_DIR / 'feature_names.json'}\")\n",
    "\n",
    "# Save model info\n",
    "model_info = {\n",
    "    'model_name': best_model_name,\n",
    "    'train_f1': final_train_f1,\n",
    "    'val_f1': final_val_f1,\n",
    "    'test_f1': final_test_f1,\n",
    "    'best_params': random_search.best_params_,\n",
    "    'n_features': len(feature_cols),\n",
    "    'n_samples': len(X),\n",
    "    'classes': le.classes_.tolist()\n",
    "}\n",
    "\n",
    "pd.DataFrame([model_info]).to_json(MODELS_DIR / 'model_info.json', orient='records', indent=2)\n",
    "print(f\"Saved model info to: {MODELS_DIR / 'model_info.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ML MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test F1-Score: {final_test_f1:.4f}\")\n",
    "print(f\"Model saved to: {MODELS_DIR}\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
